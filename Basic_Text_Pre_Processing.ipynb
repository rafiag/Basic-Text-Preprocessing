{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Text Pre-Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMh1LJSCRsKPDmBkO3vHIYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafiag/Basic-Text-Preprocessing/blob/main/Basic_Text_Pre_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOATvqySMw9L"
      },
      "source": [
        "This article will cover the basic on how to pre-process your text before feeding it into your fancy algorithm. I am going to demonstrate on how to clean your text data and how to transform it for your machine learning model using the [Bahasa Indonesia Hate Speech Dataset](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Fialfina%2Fid-hatespeech-detection%2F)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "l4g30_DJNP3-",
        "outputId": "818098f0-2879-4ad5-dadb-3b3808c3e3a9"
      },
      "source": [
        "# Import basic libs\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import warnings\r\n",
        "\r\n",
        "# Load data\r\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/rafiag/Hate-Speech-Classification/main/hate_speech_dataset.csv')\r\n",
        "\r\n",
        "print(df.shape)\r\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(713, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label                                              Tweet\n",
              "0  Non_HS  RT @spardaxyz: Fadli Zon Minta Mendagri Segera...\n",
              "1  Non_HS  RT @baguscondromowo: Mereka terus melukai aksi...\n",
              "2  Non_HS  Sylvi: bagaimana gurbernur melakukan kekerasan...\n",
              "3  Non_HS  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
              "4  Non_HS  RT @lisdaulay28: Waspada KTP palsu.....kawal P..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gCPeYrnM52m"
      },
      "source": [
        "# Text Cleaning\r\n",
        "\r\n",
        "Data pre-processing is an important step when you are trying to build a machine learning model, pre-processing process can either make or break your model. Just like any other dataset you also need to clean text data, especially because raw text data usually loaded with lots of junk or useless data. Of course you can just skip cleaning the text but it will significantly impact your model both on performance and also training time.\r\n",
        "Let's get started onto the first step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVF9661eNYtE"
      },
      "source": [
        "## Tokenize\r\n",
        "\r\n",
        "The first step we will do is tokenizing our data. Tokenizing data simply means that we will separate our data from a sentence into a list of words, for example:\r\n",
        "\r\n",
        "|  | Result |\r\n",
        "|-|-|\r\n",
        "| Original | @JohnDoe Hi, John! Today is my first day on #Twitter |\r\n",
        "| Tokenize | ['@JohnDoe', 'Hi,', 'John!', 'Today', 'is', 'my', 'first', 'day', 'on', '#Twitter'] |\r\n",
        "\r\n",
        "There are lots of way to do this. For the simplest approach you can easily use the `split()` function built-in for string in Python to achieve the same result as the example above. Or if you want more specific result, like only keeping the alphabet characters and throwing away the punctuation and numbers you can use `RegexpTokenizer()` function provided by NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "f5nLxUXIMiU-",
        "outputId": "609f807a-134c-4569-b5c8-a27ee4d879ae"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "# Simple Tokenizer\r\n",
        "def simpleTokenizer(s):\r\n",
        "    return [token for token in s.split(' ')]\r\n",
        "\r\n",
        "# Tokenize using NLTK\r\n",
        "def nltkTokenizer(s, remove_punctuation=False):\r\n",
        "    if remove_punctuation == True:\r\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "    else:\r\n",
        "        tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\r\n",
        "    return tokenizer.tokenize(s)\r\n",
        "\r\n",
        "# Tokenize words\r\n",
        "df['tokens'] = df['Tweet'].apply(nltkTokenizer)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>[RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>[RT, @baguscondromowo:, Mereka, terus, melukai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>[Sylvi, :, bagaimana, gurbernur, melakukan, ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>[RT, @lisdaulay28:, Waspada, KTP, palsu, ........</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                             tokens\n",
              "0  Non_HS  ...  [RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...\n",
              "1  Non_HS  ...  [RT, @baguscondromowo:, Mereka, terus, melukai...\n",
              "2  Non_HS  ...  [Sylvi, :, bagaimana, gurbernur, melakukan, ke...\n",
              "3  Non_HS  ...  [Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...\n",
              "4  Non_HS  ...  [RT, @lisdaulay28:, Waspada, KTP, palsu, ........\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxZePeb2OXa1",
        "outputId": "0e5d1d85-b451-429f-f147-3c0705951ed6"
      },
      "source": [
        "print('Basic approach:\\n', simpleTokenizer('@JohnDoe Hi, John! Today is my first day on #Twitter'))\r\n",
        "\r\n",
        "print('Remove non-alphabet:\\n', nltkTokenizer('@JohnDoe Hi, John! Today is my first day on #Twitter', remove_punctuation=True))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Basic approach:\n",
            " ['@JohnDoe', 'Hi,', 'John!', 'Today', 'is', 'my', 'first', 'day', 'on', '#Twitter']\n",
            "Remove non-alphabet:\n",
            " ['JohnDoe', 'Hi', 'John', 'Today', 'is', 'my', 'first', 'day', 'on', 'Twitter']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LZI4c-GUXYb"
      },
      "source": [
        "## Remove Useless Components\r\n",
        "\r\n",
        "This second step is a special one, because in most case you are not really required to do this. In this case since our data is from Twitter we will have lot of @handle, #hashtags, URLs, and other things that will just clutter our data and won't help it's performance.\r\n",
        "\r\n",
        "But again, this really depends on the case and problem you are solving. If you want to analyse the hashtag or maybe visualise the social network based on user mentions you can keep it.\r\n",
        "\r\n",
        "|  | Result |\r\n",
        "|-|-|\r\n",
        "| Tokenize | ['@JohnDoe', 'Hi,', 'John!', 'Today', 'is', 'my', 'first', 'day', 'on', '#Twitter'] |\r\n",
        "| Remove Useless Text | ['Hi', 'John', 'Today', 'is', 'my', 'first', 'day', 'on'] |\r\n",
        "\r\n",
        "Below I wrote a new function involving the re (regular expression) library to extract pattern from text using regular expression, which is really useful to remove the aforementioned elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "94YEQjTnV_2X",
        "outputId": "8122507e-406a-422b-83d9-8d47204864a9"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "def removeUselessText(tokens):\r\n",
        "    new_tokens = []\r\n",
        "    for t in tokens:\r\n",
        "        # Remove hashtag\r\n",
        "        if not t.startswith('#'):\r\n",
        "            # Remove leading & trailing whitespace\r\n",
        "            t = t.strip()\r\n",
        "            \r\n",
        "            # Remove mention\r\n",
        "            t = re.sub('@[^\\s]+', '', t)\r\n",
        "\r\n",
        "            # Remove urls\r\n",
        "            t = re.sub(r'\\\\/', '/', t) # replace escaped character\r\n",
        "            t = re.sub(r'(https?://\\S+)', '', t) # remove urls\r\n",
        "\r\n",
        "            # Remove special character and number\r\n",
        "            t = re.sub('[^a-zA-Z\\s]', '', t)\r\n",
        "\r\n",
        "            new_tokens.append(t)\r\n",
        "\r\n",
        "    return [token for token in new_tokens if token]\r\n",
        "\r\n",
        "df['no_useless'] = df['tokens'].apply(removeUselessText)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>tokens</th>\n",
              "      <th>no_useless</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>[RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...</td>\n",
              "      <td>[RT, Fadli, Zon, Minta, Mendagri, Segera, Meno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>[RT, @baguscondromowo:, Mereka, terus, melukai...</td>\n",
              "      <td>[RT, Mereka, terus, melukai, aksi, dalam, rang...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>[Sylvi, :, bagaimana, gurbernur, melakukan, ke...</td>\n",
              "      <td>[Sylvi, bagaimana, gurbernur, melakukan, keker...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, Masa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>[RT, @lisdaulay28:, Waspada, KTP, palsu, ........</td>\n",
              "      <td>[RT, Waspada, KTP, palsu, kawal, PILKADA, http...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                         no_useless\n",
              "0  Non_HS  ...  [RT, Fadli, Zon, Minta, Mendagri, Segera, Meno...\n",
              "1  Non_HS  ...  [RT, Mereka, terus, melukai, aksi, dalam, rang...\n",
              "2  Non_HS  ...  [Sylvi, bagaimana, gurbernur, melakukan, keker...\n",
              "3  Non_HS  ...  [Ahmad, Dhani, Tak, Puas, Debat, Pilkada, Masa...\n",
              "4  Non_HS  ...  [RT, Waspada, KTP, palsu, kawal, PILKADA, http...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH96Yrh0XY2L"
      },
      "source": [
        "## Stemming\r\n",
        "\r\n",
        "Onto the third step, we will stem all word into it's base form by removing the prefix and suffix of the words. Please note that this step will vary depends on the language your dataset are in. In my case because I am dealing with text in Bahasa Indonesia, the result will be something like this:\r\n",
        "\r\n",
        "|  | Result |\r\n",
        "|-|-|\r\n",
        "| Remove Useless Text | ['Hai', 'apa', 'kabar', 'aku', 'baru', 'bergabung', 'ke', 'nih'] |\r\n",
        "| Stemming | ['Hai', 'apa', 'kabar', 'aku', 'baru', 'gabung', 'ke', 'nih'] |\r\n",
        "\r\n",
        "In this step I will be using Bahasa Indonesia stemmer from Sastrawi libraries. If you are analysing text from English or other languages you can try to check on some other library such as [NLTK](https://www.nltk.org/api/nltk.stem.html?highlight=stemming).\r\n",
        "\r\n",
        "*Note: this step may take a few minutes and is a known performance problem from Sastrawi*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "4qgbd6czYht-",
        "outputId": "2148e3dc-73f5-4e98-a81f-297a854ecce8"
      },
      "source": [
        "!pip install Sastrawi\r\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\r\n",
        "\r\n",
        "def stemmSentence(tokens):\r\n",
        "    # Initiate Sastrawi stemmer\r\n",
        "    factory = StemmerFactory()\r\n",
        "    stemmer = factory.create_stemmer()\r\n",
        "\r\n",
        "    return [stemmer.stem(t) for t in tokens]\r\n",
        "\r\n",
        "df['stemmed'] = df['no_useless'].apply(stemmSentence)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>tokens</th>\n",
              "      <th>no_useless</th>\n",
              "      <th>stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>[RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...</td>\n",
              "      <td>[RT, Fadli, Zon, Minta, Mendagri, Segera, Meno...</td>\n",
              "      <td>[rt, fadli, zon, minta, mendagri, segera, nona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>[RT, @baguscondromowo:, Mereka, terus, melukai...</td>\n",
              "      <td>[RT, Mereka, terus, melukai, aksi, dalam, rang...</td>\n",
              "      <td>[rt, mereka, terus, luka, aksi, dalam, rangka,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>[Sylvi, :, bagaimana, gurbernur, melakukan, ke...</td>\n",
              "      <td>[Sylvi, bagaimana, gurbernur, melakukan, keker...</td>\n",
              "      <td>[sylvi, bagaimana, gurbernur, laku, keras, per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, Masa...</td>\n",
              "      <td>[ahmad, dhani, tak, puas, debat, pilkada, masa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>[RT, @lisdaulay28:, Waspada, KTP, palsu, ........</td>\n",
              "      <td>[RT, Waspada, KTP, palsu, kawal, PILKADA, http...</td>\n",
              "      <td>[rt, waspada, ktp, palsu, kawal, pilkada, http...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                            stemmed\n",
              "0  Non_HS  ...  [rt, fadli, zon, minta, mendagri, segera, nona...\n",
              "1  Non_HS  ...  [rt, mereka, terus, luka, aksi, dalam, rangka,...\n",
              "2  Non_HS  ...  [sylvi, bagaimana, gurbernur, laku, keras, per...\n",
              "3  Non_HS  ...  [ahmad, dhani, tak, puas, debat, pilkada, masa...\n",
              "4  Non_HS  ...  [rt, waspada, ktp, palsu, kawal, pilkada, http...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPIcCrLgacW-"
      },
      "source": [
        "## Replace Slang Words\r\n",
        "\r\n",
        "One other thing to take note of when dealing with text data in Bahasa Indonesia is slang words, Indonesian people loves to use slang, especially on the internet. Unfortunately, I am not able to found any ready-to-use Python libraries to help with this problem. \r\n",
        "\r\n",
        "Instead what I will do is use the slangword dictionary from [dhitology's GitHub repos](https://github.com/dhitology) and convert it to Python dictionaries to make replacing our data easier. Granted, this is not a really good solution since there is still some slang words in our dataset that's not included in the dictionary, but (personally) I think it's good enough.\r\n",
        "\r\n",
        "A much better approach in dealing with slang words can be found in this [article](https://medium.com/kata-engineering/mengubah-bahasa-indonesia-informal-menjadi-baku-menggunakan-kecerdasan-buatan-4c6317b00ea5) about a new methodology to transform text from informal Bahasa Indonesia to a more formal form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "YFJ_-c8Oangr",
        "outputId": "0c90131b-e84b-416b-b42e-31afa764bb13"
      },
      "source": [
        "# Load slang data\r\n",
        "slang_df = pd.read_csv('https://raw.githubusercontent.com/dhitology/sma-r/master/data/support/Slangword.csv')\r\n",
        "\r\n",
        "# Remove trailing whitespace\r\n",
        "slang_df['old'] = slang_df['old'].apply(lambda x: x.strip())\r\n",
        "slang_df['new'] = slang_df['new'].apply(lambda x: x.strip())\r\n",
        "\r\n",
        "# Transform into key value paris in a dict\r\n",
        "slang_dict = {}\r\n",
        "for idx, row in slang_df.iterrows():\r\n",
        "    slang_dict.update({row['old']: row['new']})\r\n",
        "\r\n",
        "def replaceSlang(tokens):\r\n",
        "    # iterate through tokens\r\n",
        "    for i, word in enumerate(tokens):\r\n",
        "        # check if token is in slang dictionary\r\n",
        "        try:\r\n",
        "            tokens[i] = slang_dict[word]\r\n",
        "        # if token is not slang pass\r\n",
        "        except KeyError:\r\n",
        "            pass\r\n",
        "    return tokens\r\n",
        "\r\n",
        "df['no_slang'] = df['stemmed'].apply(replaceSlang)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>tokens</th>\n",
              "      <th>no_useless</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>no_slang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>[RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...</td>\n",
              "      <td>[RT, Fadli, Zon, Minta, Mendagri, Segera, Meno...</td>\n",
              "      <td>[rt, fadli, zon, minta, mendagri, segera, nona...</td>\n",
              "      <td>[rt, fadli, zon, minta, mendagri, segera, nona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>[RT, @baguscondromowo:, Mereka, terus, melukai...</td>\n",
              "      <td>[RT, Mereka, terus, melukai, aksi, dalam, rang...</td>\n",
              "      <td>[rt, mereka, terus, luka, aksi, dalam, rangka,...</td>\n",
              "      <td>[rt, mereka, terus, luka, aksi, dalam, rangka,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>[Sylvi, :, bagaimana, gurbernur, melakukan, ke...</td>\n",
              "      <td>[Sylvi, bagaimana, gurbernur, melakukan, keker...</td>\n",
              "      <td>[sylvi, bagaimana, gurbernur, laku, keras, per...</td>\n",
              "      <td>[sylvi, bagaimana, gurbernur, laku, keras, per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, Masa...</td>\n",
              "      <td>[ahmad, dhani, tak, puas, debat, pilkada, masa...</td>\n",
              "      <td>[ahmad, dhani, tak, puas, debat, pilkada, masa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>[RT, @lisdaulay28:, Waspada, KTP, palsu, ........</td>\n",
              "      <td>[RT, Waspada, KTP, palsu, kawal, PILKADA, http...</td>\n",
              "      <td>[rt, waspada, ktp, palsu, kawal, pilkada, http...</td>\n",
              "      <td>[rt, waspada, ktp, palsu, kawal, pilkada, http...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                           no_slang\n",
              "0  Non_HS  ...  [rt, fadli, zon, minta, mendagri, segera, nona...\n",
              "1  Non_HS  ...  [rt, mereka, terus, luka, aksi, dalam, rangka,...\n",
              "2  Non_HS  ...  [sylvi, bagaimana, gurbernur, laku, keras, per...\n",
              "3  Non_HS  ...  [ahmad, dhani, tak, puas, debat, pilkada, masa...\n",
              "4  Non_HS  ...  [rt, waspada, ktp, palsu, kawal, pilkada, http...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_2Oi7hva-MT"
      },
      "source": [
        "## Remove Stop Words\r\n",
        "\r\n",
        "For the last step of our text cleaning we will remove stop words (meaningless word) from our data. I will use the Bahasa Indonesia stop words list provided in the spaCy libraries as the reference on which word to remove from our data. In this step we will also done a couple final tuning to our words:\r\n",
        "\r\n",
        "*   Transforming each token into lowercase using lower() function from Python\r\n",
        "*   Removing token less than 3 characters long in order to trim our data and increase our training speed\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "tizHkaA-bAfJ",
        "outputId": "c5a88288-585f-460b-b614-7becfa2e18b3"
      },
      "source": [
        "def removeStopWords(tokens, min_len=3):\r\n",
        "    from spacy.lang.id.stop_words import STOP_WORDS\r\n",
        "\r\n",
        "    return [t.lower() for t in tokens if t not in STOP_WORDS and len(t)>min_len]\r\n",
        "\r\n",
        "df['no_stop'] = df['no_slang'].apply(removeStopWords)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>tokens</th>\n",
              "      <th>no_useless</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>no_slang</th>\n",
              "      <th>no_stop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "      <td>[RT, @spardaxyz:, Fadli, Zon, Minta, Mendagri,...</td>\n",
              "      <td>[RT, Fadli, Zon, Minta, Mendagri, Segera, Meno...</td>\n",
              "      <td>[rt, fadli, zon, minta, mendagri, segera, nona...</td>\n",
              "      <td>[rt, fadli, zon, minta, mendagri, segera, nona...</td>\n",
              "      <td>[fadli, mendagri, nonaktif, ahok, gubernur, ht...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "      <td>[RT, @baguscondromowo:, Mereka, terus, melukai...</td>\n",
              "      <td>[RT, Mereka, terus, melukai, aksi, dalam, rang...</td>\n",
              "      <td>[rt, mereka, terus, luka, aksi, dalam, rangka,...</td>\n",
              "      <td>[rt, mereka, terus, luka, aksi, dalam, rangka,...</td>\n",
              "      <td>[luka, aksi, rangka, penjara, ahok, ahok, gaga...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "      <td>[Sylvi, :, bagaimana, gurbernur, melakukan, ke...</td>\n",
              "      <td>[Sylvi, bagaimana, gurbernur, melakukan, keker...</td>\n",
              "      <td>[sylvi, bagaimana, gurbernur, laku, keras, per...</td>\n",
              "      <td>[sylvi, bagaimana, gurbernur, laku, keras, per...</td>\n",
              "      <td>[sylvi, gurbernur, laku, keras, perempuan, buk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, ,, M...</td>\n",
              "      <td>[Ahmad, Dhani, Tak, Puas, Debat, Pilkada, Masa...</td>\n",
              "      <td>[ahmad, dhani, tak, puas, debat, pilkada, masa...</td>\n",
              "      <td>[ahmad, dhani, tak, puas, debat, pilkada, masa...</td>\n",
              "      <td>[ahmad, dhani, puas, debat, pilkada, jalan, be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "      <td>[RT, @lisdaulay28:, Waspada, KTP, palsu, ........</td>\n",
              "      <td>[RT, Waspada, KTP, palsu, kawal, PILKADA, http...</td>\n",
              "      <td>[rt, waspada, ktp, palsu, kawal, pilkada, http...</td>\n",
              "      <td>[rt, waspada, ktp, palsu, kawal, pilkada, http...</td>\n",
              "      <td>[waspada, palsu, kawal, pilkada, https, tcoooo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label  ...                                            no_stop\n",
              "0  Non_HS  ...  [fadli, mendagri, nonaktif, ahok, gubernur, ht...\n",
              "1  Non_HS  ...  [luka, aksi, rangka, penjara, ahok, ahok, gaga...\n",
              "2  Non_HS  ...  [sylvi, gurbernur, laku, keras, perempuan, buk...\n",
              "3  Non_HS  ...  [ahmad, dhani, puas, debat, pilkada, jalan, be...\n",
              "4  Non_HS  ...  [waspada, palsu, kawal, pilkada, https, tcoooo...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dkGtX39f5aO"
      },
      "source": [
        "# Splitting Dataset\r\n",
        "\r\n",
        "To finish up our pre-processing we will split our dataset into training data and testing data. Because we have a quite small dataset (about 700 rows), we will use 80% of our dataset as training set in order to give our model a little bit more data to learn from. We will also combine our tokens of word back into single string to feed it into the next step, feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvoc224igNZc",
        "outputId": "95d18c62-8bf5-434c-9299-6ae8b8f512ff"
      },
      "source": [
        "# Combine cleaned text into one string\r\n",
        "df['ready'] = df['no_stop'].apply(lambda x: ' '.join(x))\r\n",
        "\r\n",
        "# Encode target labels\r\n",
        "df['Label'] = df['Label'].apply(lambda x: 1 if x == 'HS' else 0)\r\n",
        "\r\n",
        "# Split dataset\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['ready'], \r\n",
        "                                                    df['Label'], \r\n",
        "                                                    random_state=0,\r\n",
        "                                                    train_size=0.8)\r\n",
        "\r\n",
        "print('Shape of X_train:', X_train.shape)\r\n",
        "print('Shape of X_test:', X_test.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train: (570,)\n",
            "Shape of X_test: (143,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7P-nf8AfMQN"
      },
      "source": [
        "# Feature Extraction\r\n",
        "\r\n",
        "Now that we done our cleaning can we go feed the data into our model? No, not yet, because unlike numeric data e.g. float, integer, array, and so on, your computer can't understand WORD(s). So much for artificial 'intelligence', right? (jk ily AI).\r\n",
        "\r\n",
        "So in order for our machine to understand the data we need to first translate it into something they understand, numbers. There are some methods available to transform our text data into a numerical data, such as:\r\n",
        "\r\n",
        "*   One Hot Encoding\r\n",
        "*   CountVectorizer\r\n",
        "*   TFIDF\r\n",
        "*   Word Embedding\r\n",
        "\r\n",
        "In this notebook I will use the TF-IDF vectorizer, because despite it being simpler than word embedding it can still give us a pretty good result.\r\n",
        "\r\n",
        "Other than just simply transforming our text into number there are also other features we can extract from our text, for example:\r\n",
        "\r\n",
        "*   How many words are in the text?\r\n",
        "*   How many character are in the text?\r\n",
        "*   How often non-digit character appears in the text?\r\n",
        "\r\n",
        "On this notebook I am going to stick with using our TF-IDF vector as the only feature, because from my earlier data exploration there is no notable difference of the characteristics listed above in this specific dataset. But it's always good idea to try to find out about those characteristics when you are dealing with your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mna1hVHXfh8J"
      },
      "source": [
        "## Text Vectorization\r\n",
        "\r\n",
        "To transform the text data I'm going to use the `TfidfVectorizer()` function from sklearn with the following parameters:\r\n",
        "\r\n",
        "*   min_df=2; only include words that appears in at least 2 tweets\r\n",
        "*   ngram_range=(1, 3); create a n-gram up to 3 words (trigram)\r\n",
        "\r\n",
        "`TfidfVectorizer()` also have some neat paramters such as tokenizer and stop_words that allow us to specify what tokenizer or stop_words list we want to use. But since we already pre-process the text on the previous step we won't be using those paramters.\r\n",
        "\r\n",
        "*Note: remember to ONLY fit our vectorizer into the training data to avoid data leak.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCyhkG8fn5f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "# Fit the vectorizer into training data\r\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 3)).fit(X_train)\r\n",
        "\r\n",
        "# Transform the training & testing data using the vectorizer\r\n",
        "X_train_vectorized = tfidf_vectorizer.transform(X_train)\r\n",
        "X_test_vectorized = tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_V6uEw3gq0Y"
      },
      "source": [
        "## Most Common vs. Important Words\r\n",
        "\r\n",
        "After calculating the TF-IDF vector of our data, let's take a look on the most common words (words that appears in a lot of tweets) and the most important words (words that frequently appears in single tweets). This step is important because the information can help us interpret our model later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2nGm-Fpgr8C",
        "outputId": "5bc1119a-5294-4358-a95a-df59d521c063"
      },
      "source": [
        "feature_names = np.array(tfidf_vectorizer.get_feature_names())\r\n",
        "\r\n",
        "# Sort TFIDF by value\r\n",
        "max_tf_idfs = X_train_vectorized.max(0).toarray()[0] # Get largest tfidf values across all documents.\r\n",
        "sorted_tf_idxs = max_tf_idfs.argsort() # Sorted indices\r\n",
        "sorted_tf_idfs = max_tf_idfs[sorted_tf_idxs] # Sorted TFIDF values\r\n",
        "\r\n",
        "# feature_names doesn't need to be sorted! You just access it with a list of sorted indices!\r\n",
        "smallest_tf_idfs = pd.Series(sorted_tf_idfs[:10], index=feature_names[sorted_tf_idxs[:10]])                    \r\n",
        "largest_tf_idfs = pd.Series(sorted_tf_idfs[-10:][::-1], index=feature_names[sorted_tf_idxs[-10:][::-1]])\r\n",
        "\r\n",
        "print('Most common words:\\n', smallest_tf_idfs)\r\n",
        "print('\\n')\r\n",
        "print('Most important words:\\n', largest_tf_idfs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words:\n",
            " kelola jakarta           0.192791\n",
            "kliatan kuasa problem    0.192791\n",
            "tata                     0.192791\n",
            "kelola                   0.192791\n",
            "solusi tata kelola       0.192791\n",
            "solusi tata              0.192791\n",
            "debat ahok               0.192791\n",
            "debat ahok liat          0.192791\n",
            "problem                  0.192791\n",
            "angin kliatan            0.192791\n",
            "dtype: float64\n",
            "\n",
            "\n",
            "Most important words:\n",
            " bangkit    1.0\n",
            "mati       1.0\n",
            "silvy      1.0\n",
            "serang     1.0\n",
            "fakta      1.0\n",
            "gimana     1.0\n",
            "sylvi      1.0\n",
            "hoax       1.0\n",
            "kampung    1.0\n",
            "kasar      1.0\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM5A5DJZgzCm"
      },
      "source": [
        "# Finally...\r\n",
        "\r\n",
        "With all those steps done we can now use our text dataset to build machine learning model. Similar to numeric data you can use our pre-processed text data to do classification, clustering, prediction, etc. Data cleaning and pre-processing in general might seems tedious but it definitely plays an important role when solving data science problem. This step ensure that our model will receive a good data to learn from, as they said \"a model is only as good as it's data\".\r\n",
        "\r\n",
        "# References\r\n",
        "\r\n",
        "1. Telkom Digital Talent Incubator - Data Scientist Module 7 (Text Mining)\r\n",
        "2. [Digital Business Ecosystem Research Center - Text Miining (GitHub)](https://github.com/rc-dbe/dti-tm)\r\n",
        "3. [Dhitology - Social Media Analytics (GitHub)](https://github.com/dhitology/sma-r)\r\n",
        "4. [The Dataset for Hate Speech Detection in the Indonesian Language (Bahasa Indonesia)](https://github.com/ialfina/id-hatespeech-detection/)\r\n",
        "5. [Scikit-learn Documentation](https://scikit-learn.org/stable/index.html)\r\n",
        "6. [NLTK Documentation](https://www.nltk.org/index.html)\r\n",
        "7. [spaCy Documentation](https://spacy.io/api/doc)\r\n",
        "8. [Sastrawi](https://github.com/sastrawi/sastrawi)\r\n"
      ]
    }
  ]
}